{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48540bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:25:40.220100Z",
     "start_time": "2023-04-24T10:25:39.312831Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    ToTensord,\n",
    "    EnsureChannelFirstd ,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    CropForegroundd,\n",
    "    Resized,\n",
    "    SaveImaged,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    ToDeviced\n",
    ")\n",
    "# import torch.mps\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from collections.abc import Callable, Sequence, Hashable\n",
    "from typing import Mapping,Dict\n",
    "\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.utils import first\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data.meta_tensor import MetaTensor\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.utils.misc import ImageMetaKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b3558d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:44.173178Z",
     "start_time": "2023-04-24T10:26:44.130371Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.1.0\n",
      "Numpy version: 1.24.1\n",
      "Pytorch version: 2.0.0+cu118\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3\n",
      "MONAI __file__: /home/maximus/DataspellProjects/Varian/venv/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 9.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.1+cu118\n",
      "tqdm version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.0.1\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    "    ConcatItemsd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "from monai.config import print_config\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "525b450a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'monai.deploy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmonai\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeploy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stack_images\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'monai.deploy'"
     ]
    }
   ],
   "source": [
    "from monai.deploy.core import stack_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41725e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:45.363776Z",
     "start_time": "2023-04-24T10:26:45.358478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_available = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "mps_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9f9aaed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:46.788322Z",
     "start_time": "2023-04-24T10:26:46.785246Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '/home/maximus/DataspellProjects/Varian_Hecktor'\n",
    "data_dir = 'data'\n",
    "\n",
    "train_images_ct = sorted(glob(os.path.join(data_dir, 'TrainData', '*_CT.nii.gz')))\n",
    "train_images_pt = sorted(glob(os.path.join(data_dir, 'TrainData', '*_PT.nii.gz')))\n",
    "train_labels = sorted(glob(os.path.join(data_dir, 'TrainLabels', '*.nii.gz')))\n",
    "\n",
    "trai_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(train_images_ct, train_images_pt, train_labels)]\n",
    "\n",
    "val_images_ct = sorted(glob(os.path.join(data_dir, 'ValData', '*_CT.nii.gz')))\n",
    "val_images_pt = sorted(glob(os.path.join(data_dir, 'ValData', '*_PT.nii.gz')))\n",
    "val_labels = sorted(glob(os.path.join(data_dir, 'ValLabels', '*.nii.gz')))\n",
    "val_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(val_images_ct, val_images_pt, val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc7f6c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:47.436613Z",
     "start_time": "2023-04-24T10:26:47.426914Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = []\n",
    "train_files.append(trai_files[0])\n",
    "train_files.append(trai_files[1])\n",
    "val_files = val_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac36e0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T08:49:11.987201Z",
     "start_time": "2023-04-24T08:49:11.384848Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tfiles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m tfiles\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m tfiles\u001b[38;5;241m.\u001b[39mappend(train_files[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m tfiles\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tfiles = []\n",
    "tfiles.append(train_files[3])\n",
    "tfiles.append(train_files[1])\n",
    "tfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4397133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:48.808760Z",
     "start_time": "2023-04-24T10:26:48.805574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'image': 'data/TrainData/CHUM-001__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-001__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-001.nii.gz'},\n",
       " {'image': 'data/TrainData/CHUM-002__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-002__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-002.nii.gz'},\n",
       " {'image': 'data/TrainData/CHUM-006__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-006__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-006.nii.gz'},\n",
       " {'image': 'data/TrainData/CHUM-007__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-007__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-007.nii.gz'},\n",
       " {'image': 'data/TrainData/CHUM-008__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-008__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-008.nii.gz'},\n",
       " {'image': 'data/TrainData/CHUM-013__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-013__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-013.nii.gz'},\n",
       " {'image': 'data/TrainData/CHUM-014__CT.nii.gz',\n",
       "  'image2': 'data/TrainData/CHUM-014__PT.nii.gz',\n",
       "  'label': 'data/TrainLabels/CHUM-014.nii.gz'}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files\n",
    "# val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ccc36e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:28:37.813846Z",
     "start_time": "2023-04-24T10:28:37.769701Z"
    }
   },
   "outputs": [],
   "source": [
    "class HecktorCropNeckRegion(CropForegroundd):\n",
    "    \"\"\"\n",
    "    A simple pre-processing transform to approximately crop the head and neck region based on a PET image.\n",
    "    This transform relies on several assumptions of patient orientation with a head location on the top,\n",
    "    and is specific for Hecktor22 dataset, and should not be used for an arbitrary PET image pre-processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys=[\"image\", \"image2\", \"label\"],\n",
    "        source_key=\"image\",\n",
    "        box_size=[200, 200, 310],\n",
    "        allow_missing_keys=True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(keys=keys, source_key=source_key, allow_missing_keys=allow_missing_keys, **kwargs)\n",
    "        self.box_size = box_size\n",
    "\n",
    "    def __call__(self, data : Mapping[Hashable, torch.Tensor]) -> Dict[Hashable, torch.Tensor]:\n",
    "\n",
    "        d = dict(data)\n",
    "        im_pet = d[\"image2\"][0]\n",
    "        #print(im_pet)\n",
    "        box_size = np.array(self.box_size)  # H&N region to crop in mm , defaults to 200x200x310mm\n",
    "        filename = \"\"\n",
    "\n",
    "        if isinstance(im_pet, MetaTensor):\n",
    "            filename = im_pet.meta[ImageMetaKey.FILENAME_OR_OBJ]\n",
    "            box_size = (box_size / np.array(im_pet.pixdim)).astype(int)  # compensate for resolution\n",
    "\n",
    "        box_start, box_end = self.extract_roi(im_pet=im_pet, box_size=box_size)\n",
    "        \n",
    "        if \"label\" in d and \"label\" in self.keys:\n",
    "            # if label mask is available, let's check if the cropped region includes all foreground\n",
    "            before_sum = d[\"label\"].sum().item()\n",
    "            after_sum = (\n",
    "                (d[\"label\"][0, box_start[0] : box_end[0], box_start[1] : box_end[1], box_start[2] : box_end[2]])\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "            if before_sum != after_sum:\n",
    "                print(\"WARNING, H&N crop could be incorrect!!!\", before_sum, after_sum)\n",
    "\n",
    "        d[self.start_coord_key] = box_start\n",
    "        d[self.end_coord_key] = box_end\n",
    "        \n",
    "        for key, m in self.key_iterator(d, self.mode): #question: what is mode in the iterators?\n",
    "            self.push_transform(d, key, extra_info={\"box_start\": box_start, \"box_end\": box_end})\n",
    "            d[key] = self.cropper.crop_pad(img=d[key], box_start=box_start, box_end=box_end, mode=m)\n",
    "        return d\n",
    "\n",
    "    def extract_roi(self, im_pet, box_size):\n",
    "\n",
    "        crop_len = int(0.75 * im_pet.shape[2])\n",
    "        im = im_pet[..., crop_len:]\n",
    "\n",
    "        mask = ((im - im.mean()) / im.std()) > 1\n",
    "        comp_idx = torch.argwhere(mask)\n",
    "        center = torch.mean(comp_idx.float(), dim=0).cpu().int().numpy()\n",
    "        xmin = torch.min(comp_idx, dim=0).values.cpu().int().numpy()\n",
    "        xmax = torch.max(comp_idx, dim=0).values.cpu().int().numpy()\n",
    "\n",
    "        xmin[:2] = center[:2] - box_size[:2] // 2\n",
    "        xmax[:2] = center[:2] + box_size[:2] // 2\n",
    "\n",
    "        xmax[2] = xmax[2] + crop_len\n",
    "        xmin[2] = max(0, xmax[2] - box_size[2])\n",
    "\n",
    "        return xmin.astype(int), xmax.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a33d91d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:29:21.852958Z",
     "start_time": "2023-04-24T10:29:21.807192Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'image2', 'label']),\n",
    "        EnsureChannelFirstd(['image', 'image2', 'label']),\n",
    "#         Orientationd(keys=[\"image\", \"image2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"image2\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        HecktorCropNeckRegion(keys=[\"image\", \"image2\", \"label\"], source_key=\"image\"),\n",
    "        RandSpatialCropd(keys=[\"image\", \"image2\", \"label\"], roi_size=[192, 192, 192], random_size=False),\n",
    "        ConcatItemsd(keys=[\"image\", \"image2\"], name=\"image_petct\", dim=0),\n",
    "        RandFlipd(keys=[\"image_petct\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image_petct\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image_petct\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        RandRotated(keys=[\"image_petct\", \"label\"], prob = 0.5),\n",
    "#         SaveImaged(\n",
    "#         keys = ['image_petct', 'label'],\n",
    "#         output_dir='data/output',\n",
    "#         output_postfix=\"crop\",\n",
    "#         resample=False,\n",
    "#         output_dtype=np.int16,\n",
    "#         separate_folder=False,\n",
    "#         )\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'image2', 'label']),\n",
    "        ToDeviced(keys=[\"image\", \"image2\", \"label\"], device=device),\n",
    "        EnsureChannelFirstd(['image', 'image2', 'label']),\n",
    "        Orientationd(keys=[\"image\", \"image2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"image2\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        HecktorCropNeckRegion(keys=[\"image\", \"image2\", \"label\"], source_key=\"image\"),\n",
    "        RandSpatialCropd(keys=[\"image\", \"image2\", \"label\"], roi_size=[192, 192, 192], random_size=False),\n",
    "        ConcatItemsd(keys=[\"image\", \"image2\"], name=\"image_petct\", dim=0),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2b7c8de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:29:22.858104Z",
     "start_time": "2023-04-24T10:29:22.854223Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=4)\n",
    "\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3ddbc5b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "patient = first(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "8eaf9d7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 192, 192, 192])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patient['label'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "dfd8cba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:29:25.498669Z",
     "start_time": "2023-04-24T10:29:23.334254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING, H&N crop could be incorrect!!! 9755.0 2774.0\n",
      "2023-04-27 18:17:44,610 INFO image_writer.py:194 - writing: data/output/CHUM-001__CT_crop.nii.gz\n",
      "2023-04-27 18:17:44,955 INFO image_writer.py:194 - writing: data/output/CHUM-001_crop.nii.gz\n",
      "2023-04-27 18:17:45,717 INFO image_writer.py:194 - writing: data/output/CHUM-002__CT_crop.nii.gz\n",
      "2023-04-27 18:17:46,089 INFO image_writer.py:194 - writing: data/output/CHUM-002_crop.nii.gz\n",
      "torch.Size([2, 2, 192, 192, 192])\n"
     ]
    }
   ],
   "source": [
    "for batch_data in train_loader:\n",
    "    print(batch_data['image_petct'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0ba2e3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:31:06.480757Z",
     "start_time": "2023-04-24T10:31:06.382232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.cuda.amp.grad_scaler.GradScaler object at 0x7fe78990eec0>\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4, 4],\n",
    "    spatial_dims=3,\n",
    "    init_filters=16,\n",
    "    in_channels=2,\n",
    "    out_channels=2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(192, 192, 192),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(scaler)\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d06987e6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/1\n",
      "Model Training complete\n",
      "I am here\n",
      "I am here\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [16, 2, 3, 3, 3], expected input[2, 1, 192, 192, 192] to have 2 channels, but got 1 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[59], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mamp\u001b[38;5;241m.\u001b[39mautocast():\n\u001b[0;32m---> 30\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(outputs, batch_data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     32\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/monai/networks/nets/segresnet.py:178\u001b[0m, in \u001b[0;36mSegResNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 178\u001b[0m     x, down_x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    179\u001b[0m     down_x\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    181\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecode(x, down_x)\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/monai/networks/nets/segresnet.py:155\u001b[0m, in \u001b[0;36mSegResNet.encode\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[torch\u001b[38;5;241m.\u001b[39mTensor, List[torch\u001b[38;5;241m.\u001b[39mTensor]]:\n\u001b[0;32m--> 155\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvInit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout_prob \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:613\u001b[0m, in \u001b[0;36mConv3d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    612\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 613\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:608\u001b[0m, in \u001b[0;36mConv3d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    597\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv3d(\n\u001b[1;32m    598\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    606\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[1;32m    607\u001b[0m     )\n\u001b[0;32m--> 608\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3d\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/monai/data/meta_tensor.py:268\u001b[0m, in \u001b[0;36mMetaTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    267\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 268\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__torch_function__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# if `out` has been used as argument, metadata is not copied, nothing to do.\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# if \"out\" in kwargs:\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;66;03m#     return ret\u001b[39;00m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _not_requiring_metadata(ret):\n",
      "File \u001b[0;32m~/DataspellProjects/Varian/venv/lib/python3.10/site-packages/torch/_tensor.py:1295\u001b[0m, in \u001b[0;36mTensor.__torch_function__\u001b[0;34m(cls, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _C\u001b[38;5;241m.\u001b[39mDisableTorchFunctionSubclass():\n\u001b[0;32m-> 1295\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m get_default_nowrap_functions():\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [16, 2, 3, 3, 3], expected input[2, 1, 192, 192, 192] to have 2 channels, but got 1 channels instead"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    print(\"Model Training complete\")\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        print(\"I am here\")\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        inputs, labels = (\n",
    "            batch_data[\"image_petct\"].to(device),\n",
    "            batch_data[\"label\"].to(device),\n",
    "        )\n",
    "        print(\"I am here\")\n",
    "        optimizer.zero_grad()\n",
    "#         with torch.cuda.amp.autocast():\n",
    "#             outputs = model(batch_data[\"image\"])\n",
    "#             loss = loss_function(outputs, batch_data[\"label\"])\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81719427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "e382548b1da036d5c0abb0bcbb1613c0a01166c16e19efa10a34bdcc35940384"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
