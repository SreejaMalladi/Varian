{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48540bbc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:25:40.220100Z",
     "start_time": "2023-04-24T10:25:39.312831Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "from monai.transforms import (\n",
    "    Compose,\n",
    "    LoadImaged,\n",
    "    ToTensord,\n",
    "    EnsureChannelFirstd ,\n",
    "    Orientationd,\n",
    "    Spacingd,\n",
    "    ScaleIntensityRanged,\n",
    "    CropForegroundd,\n",
    "    Resized,\n",
    "    SaveImaged,\n",
    "    RandSpatialCropd,\n",
    "    RandFlipd,\n",
    "    RandRotated,\n",
    "    ToDeviced\n",
    ")\n",
    "# import torch.mps\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nibabel as nib\n",
    "from pathlib import Path\n",
    "from collections.abc import Callable, Sequence, Hashable\n",
    "from typing import Mapping,Dict\n",
    "\n",
    "from monai.data import Dataset, DataLoader\n",
    "from monai.utils import first\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.data.meta_tensor import MetaTensor\n",
    "from monai.config.type_definitions import NdarrayOrTensor\n",
    "from monai.utils.misc import ImageMetaKey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3558d2d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:44.173178Z",
     "start_time": "2023-04-24T10:26:44.130371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONAI version: 1.1.0\n",
      "Numpy version: 1.24.1\n",
      "Pytorch version: 2.0.0+cu118\n",
      "MONAI flags: HAS_EXT = False, USE_COMPILED = False, USE_META_DICT = False\n",
      "MONAI rev id: a2ec3752f54bfc3b40e7952234fbeb5452ed63e3\n",
      "MONAI __file__: /home/maximus/DataspellProjects/Varian/venv/lib/python3.10/site-packages/monai/__init__.py\n",
      "\n",
      "Optional dependencies:\n",
      "Pytorch Ignite version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Nibabel version: 5.1.0\n",
      "scikit-image version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "Pillow version: 9.3.0\n",
      "Tensorboard version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "gdown version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "TorchVision version: 0.15.1+cu118\n",
      "tqdm version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "lmdb version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "psutil version: 5.9.5\n",
      "pandas version: 2.0.1\n",
      "einops version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "transformers version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "mlflow version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "pynrrd version: NOT INSTALLED or UNKNOWN VERSION.\n",
      "\n",
      "For details about installing the optional dependencies, please visit:\n",
      "    https://docs.monai.io/en/latest/installation.html#installing-the-recommended-dependencies\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.networks.nets import SegResNet\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "from monai.config import print_config\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b41725e7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:45.363776Z",
     "start_time": "2023-04-24T10:26:45.358478Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_available = hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available()\n",
    "mps_available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f9aaed0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:46.788322Z",
     "start_time": "2023-04-24T10:26:46.785246Z"
    }
   },
   "outputs": [],
   "source": [
    "root_dir = '/home/maximus/DataspellProjects/Varian_Hecktor'\n",
    "data_dir = 'data'\n",
    "\n",
    "train_images_ct = sorted(glob(os.path.join(data_dir, 'TrainData', '*_CT.nii.gz')))\n",
    "train_images_pt = sorted(glob(os.path.join(data_dir, 'TrainData', '*_PT.nii.gz')))\n",
    "train_labels = sorted(glob(os.path.join(data_dir, 'TrainLabels', '*.nii.gz')))\n",
    "\n",
    "trai_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(train_images_ct, train_images_pt, train_labels)]\n",
    "\n",
    "val_images_ct = sorted(glob(os.path.join(data_dir, 'ValData', '*_CT.nii.gz')))\n",
    "val_images_pt = sorted(glob(os.path.join(data_dir, 'ValData', '*_PT.nii.gz')))\n",
    "val_labels = sorted(glob(os.path.join(data_dir, 'ValLabels', '*.nii.gz')))\n",
    "val_files = [{\"image\": image_name, \"image2\": pet_image, 'label': label_name} for image_name, pet_image, label_name in zip(val_images_ct, val_images_pt, val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dc7f6c96",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:47.436613Z",
     "start_time": "2023-04-24T10:26:47.426914Z"
    }
   },
   "outputs": [],
   "source": [
    "train_files = []\n",
    "train_files.append(trai_files[0])\n",
    "train_files.append(trai_files[1])\n",
    "val_files = val_files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac36e0bc",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T08:49:11.987201Z",
     "start_time": "2023-04-24T08:49:11.384848Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m tfiles \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 2\u001b[0m tfiles\u001b[38;5;241m.\u001b[39mappend(\u001b[43mtrain_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m      3\u001b[0m tfiles\u001b[38;5;241m.\u001b[39mappend(train_files[\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      4\u001b[0m tfiles\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "tfiles = []\n",
    "tfiles.append(train_files[3])\n",
    "tfiles.append(train_files[1])\n",
    "tfiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b4397133",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:26:48.808760Z",
     "start_time": "2023-04-24T10:26:48.805574Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': 'data/ValData/CHUM-010__CT.nii.gz',\n",
       " 'image2': 'data/ValData/CHUM-010__PT.nii.gz',\n",
       " 'label': 'data/ValLabels/CHUM-010.nii.gz'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_files\n",
    "val_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ccc36e00",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:28:37.813846Z",
     "start_time": "2023-04-24T10:28:37.769701Z"
    }
   },
   "outputs": [],
   "source": [
    "class HecktorCropNeckRegion(CropForegroundd):\n",
    "    \"\"\"\n",
    "    A simple pre-processing transform to approximately crop the head and neck region based on a PET image.\n",
    "    This transform relies on several assumptions of patient orientation with a head location on the top,\n",
    "    and is specific for Hecktor22 dataset, and should not be used for an arbitrary PET image pre-processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        keys=[\"image\", \"image2\", \"label\"],\n",
    "        source_key=\"image\",\n",
    "        box_size=[200, 200, 310],\n",
    "        allow_missing_keys=True,\n",
    "        **kwargs,\n",
    "    ) -> None:\n",
    "        super().__init__(keys=keys, source_key=source_key, allow_missing_keys=allow_missing_keys, **kwargs)\n",
    "        self.box_size = box_size\n",
    "\n",
    "    def __call__(self, data : Mapping[Hashable, torch.Tensor]) -> Dict[Hashable, torch.Tensor]:\n",
    "\n",
    "        d = dict(data)\n",
    "        im_pet = d[\"image2\"][0]\n",
    "        #print(im_pet)\n",
    "        box_size = np.array(self.box_size)  # H&N region to crop in mm , defaults to 200x200x310mm\n",
    "        filename = \"\"\n",
    "\n",
    "        if isinstance(im_pet, MetaTensor):\n",
    "            filename = im_pet.meta[ImageMetaKey.FILENAME_OR_OBJ]\n",
    "            box_size = (box_size / np.array(im_pet.pixdim)).astype(int)  # compensate for resolution\n",
    "\n",
    "        box_start, box_end = self.extract_roi(im_pet=im_pet, box_size=box_size)\n",
    "        \n",
    "        if \"label\" in d and \"label\" in self.keys:\n",
    "            # if label mask is available, let's check if the cropped region includes all foreground\n",
    "            before_sum = d[\"label\"].sum().item()\n",
    "            after_sum = (\n",
    "                (d[\"label\"][0, box_start[0] : box_end[0], box_start[1] : box_end[1], box_start[2] : box_end[2]])\n",
    "                .sum()\n",
    "                .item()\n",
    "            )\n",
    "            if before_sum != after_sum:\n",
    "                print(\"WARNING, H&N crop could be incorrect!!!\", before_sum, after_sum)\n",
    "\n",
    "        d[self.start_coord_key] = box_start\n",
    "        d[self.end_coord_key] = box_end\n",
    "        \n",
    "        for key, m in self.key_iterator(d, self.mode): #question: what is mode in the iterators?\n",
    "            self.push_transform(d, key, extra_info={\"box_start\": box_start, \"box_end\": box_end})\n",
    "            d[key] = self.cropper.crop_pad(img=d[key], box_start=box_start, box_end=box_end, mode=m)\n",
    "        return d\n",
    "\n",
    "    def extract_roi(self, im_pet, box_size):\n",
    "\n",
    "        crop_len = int(0.75 * im_pet.shape[2])\n",
    "        im = im_pet[..., crop_len:]\n",
    "\n",
    "        mask = ((im - im.mean()) / im.std()) > 1\n",
    "        comp_idx = torch.argwhere(mask)\n",
    "        center = torch.mean(comp_idx.float(), dim=0).cpu().int().numpy()\n",
    "        xmin = torch.min(comp_idx, dim=0).values.cpu().int().numpy()\n",
    "        xmax = torch.max(comp_idx, dim=0).values.cpu().int().numpy()\n",
    "\n",
    "        xmin[:2] = center[:2] - box_size[:2] // 2\n",
    "        xmax[:2] = center[:2] + box_size[:2] // 2\n",
    "\n",
    "        xmax[2] = xmax[2] + crop_len\n",
    "        xmin[2] = max(0, xmax[2] - box_size[2])\n",
    "\n",
    "        return xmin.astype(int), xmax.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a33d91d3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:29:21.852958Z",
     "start_time": "2023-04-24T10:29:21.807192Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "train_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'image2', 'label']),\n",
    "        ToDeviced(keys=[\"image\", \"image2\", \"label\"], device=device),\n",
    "        EnsureChannelFirstd(['image', 'image2', 'label']),\n",
    "        Orientationd(keys=[\"image\", \"image2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"image2\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        HecktorCropNeckRegion(keys=[\"image\", \"image2\", \"label\"], source_key=\"image\"),\n",
    "        RandSpatialCropd(keys=[\"image\", \"image2\", \"label\"], roi_size=[192, 192, 192], random_size=False),\n",
    "        RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"image2\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "#         RandRotated(keys=[\"image\", \"image2\", \"label\"], prob = 0.5),\n",
    "        SaveImaged(\n",
    "        keys = ['image', 'image2', 'label'],\n",
    "        output_dir='data/output',\n",
    "        output_postfix=\"crop\",\n",
    "        resample=False,\n",
    "        output_dtype=np.int16,\n",
    "        separate_folder=False,\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "val_transforms = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image', 'image2', 'label']),\n",
    "        ToDeviced(keys=[\"image\", \"image2\", \"label\"], device=device),\n",
    "        EnsureChannelFirstd(['image', 'image2', 'label']),\n",
    "        Orientationd(keys=[\"image\", \"image2\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"image2\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        HecktorCropNeckRegion(keys=[\"image\", \"image2\", \"label\"], source_key=\"image\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2b7c8de0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:29:22.858104Z",
     "start_time": "2023-04-24T10:29:22.854223Z"
    }
   },
   "outputs": [],
   "source": [
    "train_ds = Dataset(data=train_files, transform=train_transforms)\n",
    "train_loader = DataLoader(train_ds, batch_size=1)\n",
    "\n",
    "val_ds = Dataset(data=val_files, transform=val_transforms)\n",
    "val_loader = DataLoader(val_ds, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dfd8cba9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:29:25.498669Z",
     "start_time": "2023-04-24T10:29:23.334254Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING, H&N crop could be incorrect!!! 9755.0 2774.0\n",
      "2023-04-24 10:29:24,120 INFO image_writer.py:194 - writing: data/output/CHUM-001__CT_crop.nii.gz\n",
      "2023-04-24 10:29:24,376 INFO image_writer.py:194 - writing: data/output/CHUM-001__PT_crop.nii.gz\n",
      "2023-04-24 10:29:24,436 INFO image_writer.py:194 - writing: data/output/CHUM-001_crop.nii.gz\n",
      "2023-04-24 10:29:25,189 INFO image_writer.py:194 - writing: data/output/CHUM-002__CT_crop.nii.gz\n",
      "2023-04-24 10:29:25,394 INFO image_writer.py:194 - writing: data/output/CHUM-002__PT_crop.nii.gz\n",
      "2023-04-24 10:29:25,453 INFO image_writer.py:194 - writing: data/output/CHUM-002_crop.nii.gz\n"
     ]
    }
   ],
   "source": [
    "for batch_data in train_loader:\n",
    "    batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba2e3a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-04-24T10:31:06.480757Z",
     "start_time": "2023-04-24T10:31:06.382232Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.cuda.amp.grad_scaler.GradScaler object at 0x7ff29adca350>\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 1\n",
    "val_interval = 1\n",
    "VAL_AMP = True\n",
    "\n",
    "# standard PyTorch program style: create SegResNet, DiceLoss and Adam optimizer\n",
    "\n",
    "model = SegResNet(\n",
    "    blocks_down=[1, 2, 2, 4, 4],\n",
    "    init_filters=16,\n",
    "    in_channels=1,\n",
    "    out_channels=2,\n",
    ").to(device)\n",
    "loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "optimizer = torch.optim.Adam(model.parameters(), 1e-4, weight_decay=1e-5)\n",
    "lr_scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=max_epochs)\n",
    "\n",
    "dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# define inference method\n",
    "def inference(input):\n",
    "    def _compute(input):\n",
    "        return sliding_window_inference(\n",
    "            inputs=input,\n",
    "            roi_size=(192, 192, 192),\n",
    "            sw_batch_size=1,\n",
    "            predictor=model,\n",
    "            overlap=0.5,\n",
    "        )\n",
    "\n",
    "    if VAL_AMP:\n",
    "        with torch.cuda.amp.autocast():\n",
    "            return _compute(input)\n",
    "    else:\n",
    "        return _compute(input)\n",
    "\n",
    "# use amp to accelerate training\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "print(scaler)\n",
    "# enable cuDNN benchmark\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06987e6",
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------\n",
      "epoch 1/1\n",
      "Model Training complete\n",
      "WARNING, H&N crop could be incorrect!!! 9755.0 2774.0\n",
      "2023-04-24 10:31:08,786 INFO image_writer.py:194 - writing: data/output/CHUM-001__CT_crop.nii.gz\n",
      "2023-04-24 10:31:09,036 INFO image_writer.py:194 - writing: data/output/CHUM-001__PT_crop.nii.gz\n",
      "2023-04-24 10:31:09,097 INFO image_writer.py:194 - writing: data/output/CHUM-001_crop.nii.gz\n"
     ]
    }
   ],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "best_metrics_epochs_and_time = [[], [], []]\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "metric_values_tc = []\n",
    "metric_values_wt = []\n",
    "metric_values_et = []\n",
    "\n",
    "total_start = time.time()\n",
    "for epoch in range(max_epochs):\n",
    "    epoch_start = time.time()\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    print(\"Model Training complete\")\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        print(\"I am here\")\n",
    "        step_start = time.time()\n",
    "        step += 1\n",
    "        # inputs, labels = (\n",
    "        #     batch_data[\"image\"].to(device),\n",
    "        #     batch_data[\"label\"].to(device),\n",
    "        # )\n",
    "        # print(\"I am here\")\n",
    "        optimizer.zero_grad()\n",
    "        with torch.cuda.amp.autocast():\n",
    "            outputs = model(batch_data[\"image\"])\n",
    "            loss = loss_function(outputs, batch_data[\"label\"])\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        epoch_loss += loss.item()\n",
    "        print(\n",
    "            f\"{step}/{len(train_ds) // train_loader.batch_size}\"\n",
    "            f\", train_loss: {loss.item():.4f}\"\n",
    "            f\", step time: {(time.time() - step_start):.4f}\"\n",
    "        )\n",
    "    lr_scheduler.step()\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")\n",
    "\n",
    "    if (epoch + 1) % val_interval == 0:\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for val_data in val_loader:\n",
    "                val_inputs, val_labels = (\n",
    "                    val_data[\"image\"].to(device),\n",
    "                    val_data[\"label\"].to(device),\n",
    "                )\n",
    "                val_outputs = inference(val_inputs)\n",
    "                val_outputs = [post_trans(i) for i in decollate_batch(val_outputs)]\n",
    "                dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "                dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "\n",
    "            metric = dice_metric.aggregate().item()\n",
    "            metric_values.append(metric)\n",
    "            metric_batch = dice_metric_batch.aggregate()\n",
    "            metric_tc = metric_batch[0].item()\n",
    "            metric_values_tc.append(metric_tc)\n",
    "            metric_wt = metric_batch[1].item()\n",
    "            metric_values_wt.append(metric_wt)\n",
    "            metric_et = metric_batch[2].item()\n",
    "            metric_values_et.append(metric_et)\n",
    "            dice_metric.reset()\n",
    "            dice_metric_batch.reset()\n",
    "\n",
    "            if metric > best_metric:\n",
    "                best_metric = metric\n",
    "                best_metric_epoch = epoch + 1\n",
    "                best_metrics_epochs_and_time[0].append(best_metric)\n",
    "                best_metrics_epochs_and_time[1].append(best_metric_epoch)\n",
    "                best_metrics_epochs_and_time[2].append(time.time() - total_start)\n",
    "                torch.save(\n",
    "                    model.state_dict(),\n",
    "                    os.path.join(root_dir, \"best_metric_model.pth\"),\n",
    "                )\n",
    "                print(\"saved new best metric model\")\n",
    "            print(\n",
    "                f\"current epoch: {epoch + 1} current mean dice: {metric:.4f}\"\n",
    "                f\" tc: {metric_tc:.4f} wt: {metric_wt:.4f} et: {metric_et:.4f}\"\n",
    "                f\"\\nbest mean dice: {best_metric:.4f}\"\n",
    "                f\" at epoch: {best_metric_epoch}\"\n",
    "            )\n",
    "    print(f\"time consuming of epoch {epoch + 1} is: {(time.time() - epoch_start):.4f}\")\n",
    "total_time = time.time() - total_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81719427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc-autonumbering": false,
  "vscode": {
   "interpreter": {
    "hash": "e382548b1da036d5c0abb0bcbb1613c0a01166c16e19efa10a34bdcc35940384"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
